{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neMGq8_gS5ye",
        "outputId": "dba2a04f-5f1c-4fea-fae5-fc0eefba8873"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 1): 9.998955043236686,\n",
              " (1, 2): 9.998955043236686,\n",
              " (2, 1): 9.998955043236686,\n",
              " (0, 0): 9.998955043236686,\n",
              " (3, 1): -1.0,\n",
              " (2, 0): 9.998955043236686,\n",
              " (3, 0): 9.998955043236686,\n",
              " (0, 2): 9.998955043236686,\n",
              " (2, 2): 9.998955043236686,\n",
              " (1, 0): 9.998955043236686,\n",
              " (3, 2): 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# from utils import vector_add, orientations, turn_right, turn_left\n",
        "import operator\n",
        "\n",
        "def vector_add(a, b):\n",
        "    \"\"\"Component-wise addition of two vectors.\"\"\"\n",
        "    return tuple(map(operator.add, a, b))\n",
        "\n",
        "orientations = EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "turns = LEFT, RIGHT = (+1, -1)\n",
        "\n",
        "def turn_heading(heading, inc, headings=orientations):\n",
        "    # print(\"AFTER TURN\")\n",
        "    # print(headings[(headings.index(heading) + inc) % len(headings)])\n",
        "    return headings[(headings.index(heading) + inc) % len(headings)]\n",
        "\n",
        "\n",
        "def turn_right(heading):\n",
        "    # print(heading)\n",
        "    # print(\"RIGHT\")\n",
        "    return turn_heading(heading, RIGHT)\n",
        "\n",
        "\n",
        "def turn_left(heading):\n",
        "    # print(heading)\n",
        "    # print(\"LEFT\")\n",
        "    return turn_heading(heading, LEFT)\n",
        "\n",
        "\n",
        "class MDP:\n",
        "\n",
        "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
        "        if not (0 < gamma <= 1):\n",
        "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
        "\n",
        "        # collect states from transitions table if not passed.\n",
        "        self.states = states or self.get_states_from_transitions(transitions)\n",
        "\n",
        "        self.init = init\n",
        "\n",
        "        if isinstance(actlist, list):\n",
        "            # if actlist is a list, all states have the same actions\n",
        "            self.actlist = actlist\n",
        "\n",
        "        elif isinstance(actlist, dict):\n",
        "            # if actlist is a dict, different actions for each state\n",
        "            self.actlist = actlist\n",
        "\n",
        "        self.terminals = terminals\n",
        "        self.transitions = transitions or {}\n",
        "        if not self.transitions:\n",
        "            print(\"Warning: Transition table is empty.\")\n",
        "\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.reward = reward or {s: 0 for s in self.states}\n",
        "\n",
        "        # self.check_consistency()\n",
        "\n",
        "    def R(self, state):\n",
        "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
        "\n",
        "        return self.reward[state]\n",
        "\n",
        "    def T(self, state, action):\n",
        "        \"\"\"Transition model. From a state and an action, return a list\n",
        "        of (probability, result-state) pairs.\"\"\"\n",
        "\n",
        "        if not self.transitions:\n",
        "            raise ValueError(\"Transition model is missing\")\n",
        "        else:\n",
        "            return self.transitions[state][action]\n",
        "\n",
        "    def actions(self, state):\n",
        "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
        "        fixed list of actions, except for terminal states. Override this\n",
        "        method if you need to specialize by state.\"\"\"\n",
        "\n",
        "        if state in self.terminals:\n",
        "            return [None]\n",
        "        else:\n",
        "            return self.actlist\n",
        "\n",
        "    def get_states_from_transitions(self, transitions):\n",
        "        if isinstance(transitions, dict):\n",
        "            s1 = set(transitions.keys())\n",
        "            s2 = set(tr[1] for actions in transitions.values()\n",
        "                     for effects in actions.values()\n",
        "                     for tr in effects)\n",
        "            return s1.union(s2)\n",
        "        else:\n",
        "            print('Could not retrieve states from transitions')\n",
        "            return None\n",
        "\n",
        "    def check_consistency(self):\n",
        "\n",
        "        # check that all states in transitions are valid\n",
        "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
        "\n",
        "        # check that init is a valid state\n",
        "        assert self.init in self.states\n",
        "\n",
        "        # check reward for each state\n",
        "        assert set(self.reward.keys()) == set(self.states)\n",
        "\n",
        "        # check that all terminals are valid states\n",
        "        assert all(t in self.states for t in self.terminals)\n",
        "\n",
        "        # check that probability distributions for all actions sum to 1\n",
        "        for s1, actions in self.transitions.items():\n",
        "            for a in actions.keys():\n",
        "                s = 0\n",
        "                for o in actions[a]:\n",
        "                    s += o[0]\n",
        "                assert abs(s - 1) < 0.001\n",
        "\n",
        "\n",
        "class MDP2(MDP):\n",
        "    \"\"\"\n",
        "    Inherits from MDP. Handles terminal states, and transitions to and from terminal states better.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, init, actlist, terminals, transitions, reward=None, gamma=0.9):\n",
        "        MDP.__init__(self, init, actlist, terminals, transitions, reward, gamma=gamma)\n",
        "\n",
        "    def T(self, state, action):\n",
        "        if action is None:\n",
        "            return [(0.0, state)]\n",
        "        else:\n",
        "            return self.transitions[state][action]\n",
        "\n",
        "\n",
        "class GridMDP(MDP):\n",
        "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1]. All you have to do is\n",
        "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
        "    (unreachable state). Also, you should specify the terminal states.\n",
        "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
        "\n",
        "    def __init__(self, grid, terminals, init=(0, 0), gamma=0.9):\n",
        "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
        "        reward = {}\n",
        "        states = set()\n",
        "        self.rows = len(grid)\n",
        "        self.cols = len(grid[0])\n",
        "        self.grid = grid\n",
        "        for x in range(self.cols):\n",
        "            for y in range(self.rows):\n",
        "                if grid[y][x]:\n",
        "                    states.add((x, y))\n",
        "                    reward[(x, y)] = grid[y][x]\n",
        "        self.states = states\n",
        "        actlist = orientations\n",
        "        # print(actlist)  \n",
        "        transitions = {}\n",
        "        for s in states:\n",
        "            transitions[s] = {}\n",
        "            for a in actlist:\n",
        "                transitions[s][a] = self.calculate_T(s, a)\n",
        "                # print(s, a)\n",
        "                # print( transitions[s][a])\n",
        "                # print(\"\\n\")\n",
        "        # print(transitions)\n",
        "        MDP.__init__(self, init, actlist=actlist,\n",
        "                     terminals=terminals, transitions=transitions,\n",
        "                     reward=reward, states=states, gamma=gamma)\n",
        "\n",
        "    def calculate_T(self, state, action):\n",
        "        if action:\n",
        "            return [(0.8, self.go(state, action)),\n",
        "                    (0.1, self.go(state, turn_right(action))),\n",
        "                    (0.1, self.go(state, turn_left(action)))]\n",
        "        else:\n",
        "            return [(0.0, state)]\n",
        "\n",
        "    def T(self, state, action):\n",
        "        return self.transitions[state][action] if action else [(0.0, state)]\n",
        "\n",
        "    def go(self, state, direction):\n",
        "        \"\"\"Return the state that results from going in this direction.\"\"\"\n",
        "\n",
        "        state1 = vector_add(state, direction)\n",
        "        return state1 if state1 in self.states else state\n",
        "\n",
        "    def to_grid(self, mapping):\n",
        "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
        "\n",
        "        return list(reversed([[mapping.get((x, y), None)\n",
        "                               for x in range(self.cols)]\n",
        "                              for y in range(self.rows)]))\n",
        "\n",
        "    def to_arrows(self, policy):\n",
        "        chars = {(1, 0): 'R', (0, 1): 'U', (-1, 0): 'L', (0, -1): 'D', None: 'O'}\n",
        "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
        "\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "\n",
        "\n",
        "\"\"\" [Figure 17.1]\n",
        "A 4x3 grid environment that presents the agent with a sequential decision problem.\n",
        "\"\"\"\n",
        "\n",
        "sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n",
        "                                           [-0.04, None, -0.04, -1],\n",
        "                                           [-0.04, -0.04, -0.04, -0.04]],\n",
        "                                          terminals=[(3, 2), (3, 1)])\n",
        "\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "\n",
        "\n",
        "def value_iteration(mdp, epsilon=0.001):\n",
        "    \"\"\"Solving an MDP by value iteration. [Figure 17.4]\"\"\"\n",
        "\n",
        "    U1 = {s: 0 for s in mdp.states}\n",
        "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
        "    while True:\n",
        "        U = U1.copy()\n",
        "        delta = 0\n",
        "        for s in mdp.states:\n",
        "            U1[s] = R(s) + gamma * max(sum(p * U[s1] for (p, s1) in T(s, a))\n",
        "                                       for a in mdp.actions(s))\n",
        "            delta = max(delta, abs(U1[s] - U[s]))\n",
        "        if delta <= epsilon * (1 - gamma) / gamma:\n",
        "            return U\n",
        "\n",
        "\n",
        "def best_policy(mdp, U):\n",
        "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
        "    as a mapping from state to action. [Equation 17.4]\"\"\"\n",
        "\n",
        "    pi = {}\n",
        "    for s in mdp.states:\n",
        "        pi[s] = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
        "    return pi\n",
        "\n",
        "\n",
        "def expected_utility(a, s, U, mdp):\n",
        "    \"\"\"The expected utility of doing a in state s, according to the MDP and U.\"\"\"\n",
        "\n",
        "    return sum(p * U[s1] for (p, s1) in mdp.T(s, a))\n",
        "\n",
        "# ______________________________________________________________________________\n",
        "\n",
        "value_iteration(sequential_decision_environment, epsilon=0.001)\n",
        "# for reward -2\n",
        "sequential_decision_environment = GridMDP([[-2, -2, -2, +1],\n",
        "                                           [-2, None, -2, -1],\n",
        "                                           [-2, -2, -2, -2]],\n",
        "                                          terminals=[(3, 2), (3, 1)])\n",
        "value_iteration(sequential_decision_environment, epsilon=0.001)\n",
        "\n",
        "#for reward 0.1\n",
        "sequential_decision_environment = GridMDP([[0.1, 0.1, 0.1, +1],\n",
        "                                           [0.1, None, 0.1, -1],\n",
        "                                           [0.1, 0.1, 0.1, 0.1]],\n",
        "                                          terminals=[(3, 2), (3, 1)])\n",
        "value_iteration(sequential_decision_environment, epsilon=0.001)\n",
        "\n",
        "#for reward 0.02\n",
        "sequential_decision_environment = GridMDP([[0.02, 0.02, 0.02, +1],\n",
        "                                           [0.02, None, 0.02, -1],\n",
        "                                           [0.02, 0.02, 0.02, 0.02]],\n",
        "                                          terminals=[(3, 2), (3, 1)])\n",
        "value_iteration(sequential_decision_environment, epsilon=0.001)\n",
        "\n",
        "#for reward 1\n",
        "sequential_decision_environment = GridMDP([[1, 1, 1, +1],\n",
        "                                           [1, None, 1, -1],\n",
        "                                           [1, 1, 1, 1]],\n",
        "                                          terminals=[(3, 2), (3, 1)])\n",
        "value_iteration(sequential_decision_environment, epsilon=0.001)\n"
      ]
    }
  ]
}